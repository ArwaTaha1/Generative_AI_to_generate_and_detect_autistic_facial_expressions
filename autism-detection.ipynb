{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1024609,"sourceType":"datasetVersion","datasetId":564001},{"sourceId":11346343,"sourceType":"datasetVersion","datasetId":7099464},{"sourceId":11426369,"sourceType":"datasetVersion","datasetId":6808767},{"sourceId":11476255,"sourceType":"datasetVersion","datasetId":7192569},{"sourceId":11827939,"sourceType":"datasetVersion","datasetId":7367041},{"sourceId":329888,"sourceType":"modelInstanceVersion","modelInstanceId":253213,"modelId":274673},{"sourceId":340472,"sourceType":"modelInstanceVersion","modelInstanceId":284728,"modelId":305570}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np  \nimport pandas as pd\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport time\nimport matplotlib.pyplot as plt\nimport cv2\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport shutil\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax, AdamW\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nimport time\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications import ResNet50\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:37:36.575804Z","iopub.execute_input":"2025-05-16T13:37:36.576337Z","iopub.status.idle":"2025-05-16T13:37:50.441661Z","shell.execute_reply.started":"2025-05-16T13:37:36.576313Z","shell.execute_reply":"2025-05-16T13:37:50.441089Z"}},"outputs":[{"name":"stderr","text":"2025-05-16 13:37:39.521412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747402659.718524      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747402659.774548      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def make_dataframes(sdir):\n    dataset_names= ['train', 'test' ,'valid'] \n    train_path=os.path.join(sdir, 'train') #/kaggle/input/autism/autism/(train or test or valid)\n    test_path=os.path.join(sdir, 'test')\n    valid_path=os.path.join(sdir, 'valid')\n    path_list=[train_path, test_path, valid_path]   \n    zip_list=zip(dataset_names, path_list) \n    \n    # generate train_df, test_df, valid_df\n    for dataset, setpath in zip_list:\n        filepaths=[]\n        labels=[]\n        classes=sorted(os.listdir(setpath))\n        for klass in classes:\n            classpath=os.path.join(setpath, klass)\n            flist=sorted(os.listdir(classpath))           \n            desc=f'{dataset:6s}-{klass:13s}'            \n            for f in tqdm(flist, ncols=130, desc=desc):\n                fpath=os.path.join(classpath,f)\n                filepaths.append(fpath)\n                labels.append(klass)\n        Fseries=pd.Series(filepaths, name='filepaths')\n        Lseries=pd.Series(labels, name='labels')\n        df=pd.concat([Fseries, Lseries], axis=1)\n        if dataset =='train':\n            train_df=df\n        elif dataset == 'test':\n            test_df=df\n        else:\n            valid_df=df\n    classes=sorted(train_df['labels'].unique())\n    class_count=len(classes)\n    sample_df=train_df.sample(n=50, replace=False)\n    # calculate the average image height and with\n    ht=0\n    wt=0\n    count=0\n    for i in range(len(sample_df)):\n        fpath=sample_df['filepaths'].iloc[i]\n        try:\n            img=cv2.imread(fpath)\n            h=img.shape[0]\n            w=img.shape[1]\n            wt +=w\n            ht +=h\n            count +=1\n        except:\n            pass\n    have=int(ht/count)\n    wave=int(wt/count)\n    aspect_ratio=have/wave\n    print('number of classes in processed dataset= ', class_count)    \n    counts=list(train_df['labels'].value_counts())    \n    print('the maximum files in any class in train_df is ', max(counts), '  the minimum files in any class in train_df is ', min(counts))\n    print('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))  \n    print('average image height= ', have, '  average image width= ', wave, ' aspect ratio h/w= ', aspect_ratio)\n    return train_df, test_df, valid_df, classes, class_count, sdir\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:38:05.227655Z","iopub.execute_input":"2025-05-16T13:38:05.228548Z","iopub.status.idle":"2025-05-16T13:38:05.237570Z","shell.execute_reply.started":"2025-05-16T13:38:05.228520Z","shell.execute_reply":"2025-05-16T13:38:05.236888Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df, test_df, valid_df, classes, class_count, sdir=make_dataframes(\"/kaggle/input/autism-modified/autism\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:38:09.794190Z","iopub.execute_input":"2025-05-16T13:38:09.794529Z","iopub.status.idle":"2025-05-16T13:38:10.607943Z","shell.execute_reply.started":"2025-05-16T13:38:09.794506Z","shell.execute_reply":"2025-05-16T13:38:10.607126Z"}},"outputs":[{"name":"stderr","text":"train -autistic     : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1228/1228 [00:00<00:00, 1080017.89it/s]\ntrain -non_autistic : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1246/1246 [00:00<00:00, 1064813.12it/s]\ntest  -autistic     : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 423239.56it/s]\ntest  -non_autistic : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 462947.46it/s]\nvalid -autistic     : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 701388.63it/s]\nvalid -non_autistic : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 550433.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"number of classes in processed dataset=  2\nthe maximum files in any class in train_df is  1246   the minimum files in any class in train_df is  1228\ntrain_df length:  2474   test_df length:  200   valid_df length:  200\naverage image height=  333   average image width=  288  aspect ratio h/w=  1.15625\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def make_gens(batch_size, train_df, test_df, valid_df, img_size):\n    \n    trgen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=30,\n        zoom_range=0.3,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        brightness_range=[0.7, 1.3],\n        horizontal_flip=True,\n        shear_range=0.2,\n        fill_mode='nearest'\n    )\n    t_and_v_gen=ImageDataGenerator(rescale=1./255)\n\n    msg='{0:70s} for train generator'.format(' ')\n    print(msg, '\\r', end='') # prints over on the same line\n    \n    train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n                                       class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n    \n    msg='{0:70s} for valid generator'.format(' ')\n    print(msg, '\\r', end='') # prints over on the same line\n    \n    valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n    \n    # for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set\n    # this ensures that we go through all the sample in the test set exactly once.\n    \n    length=len(test_df)\n    test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n    test_steps=int(length/test_batch_size)\n    \n    msg='{0:70s} for test generator'.format(' ')\n    print(msg, '\\r', end='') # prints over on the same line\n    \n    test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n\n    \n    # from the generator we can get information we will need later\n    classes=list(train_gen.class_indices.keys())\n    class_indices=list(train_gen.class_indices.values())\n    class_count=len(classes)\n    labels=test_gen.labels\n    print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)\n    return train_gen, test_gen, valid_gen, test_batch_size, test_steps, classes\n\n\nbatch_size=20\nimg_size=(224,224)\ntrain_gen, test_gen, valid_gen, test_batch_size, test_steps, classes=make_gens(batch_size, train_df, test_df, valid_df, img_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:38:13.526620Z","iopub.execute_input":"2025-05-16T13:38:13.527196Z","iopub.status.idle":"2025-05-16T13:38:25.640912Z","shell.execute_reply.started":"2025-05-16T13:38:13.527161Z","shell.execute_reply":"2025-05-16T13:38:25.640145Z"}},"outputs":[{"name":"stdout","text":"Found 2474 validated image filenames belonging to 2 classes.           for train generator \nFound 200 validated image filenames belonging to 2 classes.            for valid generator \nFound 200 validated image filenames belonging to 2 classes.            for test generator \ntest batch size:  50   test steps:  4  number of classes :  2\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def make_model(img_size):\n  #  print('Enter the initial model learning rate. I recommend .002', flush=True)\n  #  lr=float(input(' '))\n    img_shape=(img_size[0], img_size[1], 3)\n    base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n    # Note you are always told NOT to make the base model trainable initially- that is WRONG you get better results leaving it trainable\n    base_model.trainable=True\n    l2 = regularizers.l2(9e-6)\n    x = base_model.output\n    # x = BatchNormalization()(x)\n    # x = Dense(512, activation='relu', kernel_regularizer=l2)(x)\n    # x = BatchNormalization()(x)\n    # x = Dropout(0.5)(x)\n    # x = Dense(128, activation='relu', kernel_regularizer=l2)(x)\n    # x = BatchNormalization()(x)\n    # x = Dropout(0.4)(x)\n    x = BatchNormalization()(x)\n    x = Dense(512, activation='relu', kernel_regularizer=l2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(256, activation='relu', kernel_regularizer=l2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    x = Dense(128, activation='relu', kernel_regularizer=l2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    output = Dense(class_count, activation='softmax')(x)\n    \n    model=Model(inputs=base_model.input, outputs=output)\n    loss = CategoricalCrossentropy(label_smoothing=0.05)\n    model.compile(Adamax(learning_rate=0.0015), loss=loss, metrics=['accuracy'])\n    return model\n\nmodel=make_model(img_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:14:07.709712Z","iopub.execute_input":"2025-05-16T14:14:07.710350Z","iopub.status.idle":"2025-05-16T14:14:09.245860Z","shell.execute_reply.started":"2025-05-16T14:14:07.710324Z","shell.execute_reply":"2025-05-16T14:14:09.245082Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\n\nclass TestAccuracyEarlyStopping(Callback):\n    def __init__(self, test_data, patience=5):\n        super(TestAccuracyEarlyStopping, self).__init__()\n        self.test_data = test_data\n        self.patience = patience\n        self.best_val_accuracy = 0\n        self.best_test_accuracy = 0\n        self.epochs_no_improve = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Get validation accuracy from logs\n        val_accuracy = logs.get('val_accuracy')\n\n        # Evaluate the model on the test data\n        test_loss, test_accuracy = self.model.evaluate(self.test_data, verbose=0)\n\n        # Check if validation accuracy has improved\n        if val_accuracy > self.best_val_accuracy:\n            self.best_val_accuracy = val_accuracy\n            \n            # Check if test accuracy has decreased\n            if test_accuracy < self.best_test_accuracy:\n                self.epochs_no_improve += 1\n                print(f\"\\nğŸš¨ Test accuracy dropped to {test_accuracy:.4f} from {self.best_test_accuracy:.4f}\")\n            else:\n                self.best_test_accuracy = test_accuracy\n                self.epochs_no_improve = 0\n        else:\n            self.epochs_no_improve += 1\n        \n        # Stop training if patience is exceeded\n        if self.epochs_no_improve >= self.patience:\n            print(f\"\\nğŸ›‘ Early stopping: Test accuracy did not improve for {self.patience} consecutive epochs.\")\n            self.model.stop_training = True\n\n        print(f\"âœ… Epoch {epoch+1}: Val Accuracy = {val_accuracy:.4f}, Test Accuracy = {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T23:53:36.854036Z","iopub.execute_input":"2025-05-15T23:53:36.854797Z","iopub.status.idle":"2025-05-15T23:53:36.862985Z","shell.execute_reply.started":"2025-05-15T23:53:36.854774Z","shell.execute_reply":"2025-05-15T23:53:36.862358Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('/kaggle/working/model_2.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1, mode='min')\n#test_early_stopping = TestAccuracyEarlyStopping(test_data=test_gen, patience=15)\nreduce_lr  = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\nepochs = 40","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:18:38.067258Z","iopub.execute_input":"2025-05-16T14:18:38.067539Z","iopub.status.idle":"2025-05-16T14:18:38.072310Z","shell.execute_reply.started":"2025-05-16T14:18:38.067518Z","shell.execute_reply":"2025-05-16T14:18:38.071658Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"history=model.fit(x=train_gen,  epochs=epochs, verbose=1,   validation_data=valid_gen,\n                initial_epoch=0, callbacks=[early_stop,reduce_lr,checkpoint])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:18:41.730277Z","iopub.execute_input":"2025-05-16T14:18:41.730828Z","iopub.status.idle":"2025-05-16T14:33:51.173090Z","shell.execute_reply.started":"2025-05-16T14:18:41.730807Z","shell.execute_reply":"2025-05-16T14:33:51.171189Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.6838 - loss: 0.7221\nEpoch 1: val_loss improved from inf to 0.77267, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 287ms/step - accuracy: 0.6838 - loss: 0.7221 - val_accuracy: 0.5850 - val_loss: 0.7727 - learning_rate: 0.0015\nEpoch 2/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.7200 - loss: 0.6875\nEpoch 2: val_loss did not improve from 0.77267\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 267ms/step - accuracy: 0.7199 - loss: 0.6873 - val_accuracy: 0.6950 - val_loss: 0.8012 - learning_rate: 0.0015\nEpoch 3/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.7450 - loss: 0.6272\nEpoch 3: val_loss improved from 0.77267 to 0.67722, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 283ms/step - accuracy: 0.7451 - loss: 0.6272 - val_accuracy: 0.7800 - val_loss: 0.6772 - learning_rate: 0.0015\nEpoch 4/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.7508 - loss: 0.6111\nEpoch 4: val_loss improved from 0.67722 to 0.58629, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 282ms/step - accuracy: 0.7508 - loss: 0.6110 - val_accuracy: 0.7900 - val_loss: 0.5863 - learning_rate: 0.0015\nEpoch 5/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.7656 - loss: 0.5805\nEpoch 5: val_loss improved from 0.58629 to 0.55320, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 283ms/step - accuracy: 0.7657 - loss: 0.5804 - val_accuracy: 0.7700 - val_loss: 0.5532 - learning_rate: 0.0015\nEpoch 6/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.7910 - loss: 0.5507\nEpoch 6: val_loss improved from 0.55320 to 0.47422, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 283ms/step - accuracy: 0.7910 - loss: 0.5506 - val_accuracy: 0.8550 - val_loss: 0.4742 - learning_rate: 0.0015\nEpoch 7/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.8203 - loss: 0.5231\nEpoch 7: val_loss did not improve from 0.47422\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 270ms/step - accuracy: 0.8202 - loss: 0.5232 - val_accuracy: 0.8450 - val_loss: 0.4855 - learning_rate: 0.0015\nEpoch 8/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.8235 - loss: 0.5176\nEpoch 8: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n\nEpoch 8: val_loss did not improve from 0.47422\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 268ms/step - accuracy: 0.8235 - loss: 0.5176 - val_accuracy: 0.8500 - val_loss: 0.4833 - learning_rate: 0.0015\nEpoch 9/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.8511 - loss: 0.4792\nEpoch 9: val_loss improved from 0.47422 to 0.46972, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 277ms/step - accuracy: 0.8510 - loss: 0.4792 - val_accuracy: 0.8400 - val_loss: 0.4697 - learning_rate: 7.5000e-04\nEpoch 10/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.8459 - loss: 0.4685\nEpoch 10: val_loss improved from 0.46972 to 0.45894, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 281ms/step - accuracy: 0.8459 - loss: 0.4685 - val_accuracy: 0.8550 - val_loss: 0.4589 - learning_rate: 7.5000e-04\nEpoch 11/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.8463 - loss: 0.4639\nEpoch 11: val_loss did not improve from 0.45894\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 266ms/step - accuracy: 0.8463 - loss: 0.4638 - val_accuracy: 0.8450 - val_loss: 0.4760 - learning_rate: 7.5000e-04\nEpoch 12/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.8550 - loss: 0.4414\nEpoch 12: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n\nEpoch 12: val_loss did not improve from 0.45894\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 264ms/step - accuracy: 0.8550 - loss: 0.4414 - val_accuracy: 0.8450 - val_loss: 0.4955 - learning_rate: 7.5000e-04\nEpoch 13/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.8627 - loss: 0.4327\nEpoch 13: val_loss improved from 0.45894 to 0.45666, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 281ms/step - accuracy: 0.8628 - loss: 0.4326 - val_accuracy: 0.8700 - val_loss: 0.4567 - learning_rate: 3.7500e-04\nEpoch 14/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.8814 - loss: 0.4031\nEpoch 14: val_loss did not improve from 0.45666\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 276ms/step - accuracy: 0.8814 - loss: 0.4031 - val_accuracy: 0.8550 - val_loss: 0.4767 - learning_rate: 3.7500e-04\nEpoch 15/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.8863 - loss: 0.4055\nEpoch 15: val_loss improved from 0.45666 to 0.45330, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 288ms/step - accuracy: 0.8863 - loss: 0.4056 - val_accuracy: 0.8700 - val_loss: 0.4533 - learning_rate: 3.7500e-04\nEpoch 16/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.8859 - loss: 0.4053\nEpoch 16: val_loss did not improve from 0.45330\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 272ms/step - accuracy: 0.8859 - loss: 0.4052 - val_accuracy: 0.8650 - val_loss: 0.4739 - learning_rate: 3.7500e-04\nEpoch 17/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.8867 - loss: 0.3871\nEpoch 17: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n\nEpoch 17: val_loss did not improve from 0.45330\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 267ms/step - accuracy: 0.8868 - loss: 0.3870 - val_accuracy: 0.8550 - val_loss: 0.4690 - learning_rate: 3.7500e-04\nEpoch 18/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9140 - loss: 0.3581\nEpoch 18: val_loss improved from 0.45330 to 0.43786, saving model to /kaggle/working/model_2.keras\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 279ms/step - accuracy: 0.9140 - loss: 0.3581 - val_accuracy: 0.8800 - val_loss: 0.4379 - learning_rate: 1.8750e-04\nEpoch 19/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.8957 - loss: 0.3806\nEpoch 19: val_loss did not improve from 0.43786\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 267ms/step - accuracy: 0.8958 - loss: 0.3805 - val_accuracy: 0.8700 - val_loss: 0.4676 - learning_rate: 1.8750e-04\nEpoch 20/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.9209 - loss: 0.3413\nEpoch 20: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n\nEpoch 20: val_loss did not improve from 0.43786\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 268ms/step - accuracy: 0.9208 - loss: 0.3413 - val_accuracy: 0.8650 - val_loss: 0.4560 - learning_rate: 1.8750e-04\nEpoch 21/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9258 - loss: 0.3358\nEpoch 21: val_loss did not improve from 0.43786\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 267ms/step - accuracy: 0.9258 - loss: 0.3359 - val_accuracy: 0.8750 - val_loss: 0.4593 - learning_rate: 9.3750e-05\nEpoch 22/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.9061 - loss: 0.3568\nEpoch 22: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n\nEpoch 22: val_loss did not improve from 0.43786\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 271ms/step - accuracy: 0.9061 - loss: 0.3568 - val_accuracy: 0.8650 - val_loss: 0.4772 - learning_rate: 9.3750e-05\nEpoch 23/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9230 - loss: 0.3345\nEpoch 23: val_loss did not improve from 0.43786\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 267ms/step - accuracy: 0.9230 - loss: 0.3345 - val_accuracy: 0.8500 - val_loss: 0.4876 - learning_rate: 4.6875e-05\nEpoch 24/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9289 - loss: 0.3290\nEpoch 24: ReduceLROnPlateau reducing learning rate to 2.3437500203726813e-05.\n\nEpoch 24: val_loss did not improve from 0.43786\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 266ms/step - accuracy: 0.9288 - loss: 0.3291 - val_accuracy: 0.8600 - val_loss: 0.4737 - learning_rate: 4.6875e-05\nEpoch 25/40\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9375 - loss: 0.3212\nEpoch 25: val_loss did not improve from 0.43786\n\u001b[1m124/124\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 267ms/step - accuracy: 0.9374 - loss: 0.3212 - val_accuracy: 0.8600 - val_loss: 0.4701 - learning_rate: 2.3438e-05\nEpoch 26/40\n\u001b[1m 34/124\u001b[0m \u001b[32mâ”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m24s\u001b[0m 272ms/step - accuracy: 0.9200 - loss: 0.3393","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/327299117.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history=model.fit(x=train_gen,  epochs=epochs, verbose=1,   validation_data=valid_gen,\n\u001b[0m\u001b[1;32m      2\u001b[0m                 initial_epoch=0, callbacks=[early_stop,reduce_lr,checkpoint])\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"x_val, y_val = next(valid_gen)\npreds = model.predict(x_val)\npred_classes = np.argmax(preds, axis=1)\ntrue_classes = np.argmax(y_val, axis=1)\n\nprint(\"Predictions:\", pred_classes[:20])\nprint(\"True Labels:\", true_classes[:20])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:49:16.907855Z","iopub.status.idle":"2025-05-15T18:49:16.908194Z","shell.execute_reply.started":"2025-05-15T18:49:16.907987Z","shell.execute_reply":"2025-05-15T18:49:16.908010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_loss, test_acc = model.evaluate(test_gen)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:01:13.492240Z","iopub.execute_input":"2025-05-16T14:01:13.492823Z","iopub.status.idle":"2025-05-16T14:01:22.709416Z","shell.execute_reply.started":"2025-05-16T14:01:13.492800Z","shell.execute_reply":"2025-05-16T14:01:22.708761Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 357ms/step - accuracy: 0.8787 - loss: 0.3389\nTest Accuracy: 90.00%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def predictor(test_gen, test_steps):\n    y_pred= []\n    y_true=test_gen.labels\n    classes=list(test_gen.class_indices.keys())\n    class_count=len(classes)\n    errors=0\n    preds=model.predict(test_gen, verbose=1)\n    tests=len(preds)    \n    for i, p in enumerate(preds):        \n        pred_index=np.argmax(p)         \n        true_index=test_gen.labels[i]  # labels are integer values        \n        if pred_index != true_index: # a misclassification has occurred                                           \n            errors=errors + 1\n            file=test_gen.filenames[i]            \n        y_pred.append(pred_index)\n            \n    acc=( 1-errors/tests) * 100\n    print(f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}')\n    ypred=np.array(y_pred)\n    ytrue=np.array(y_true)\n    if class_count <=30:\n        cm = confusion_matrix(ytrue, ypred )\n        # plot the confusion matrix\n        plt.figure(figsize=(12, 8))\n        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n    clr = classification_report(y_true, y_pred, target_names=classes, digits= 4) # create classification report\n    print(\"Classification Report:\\n----------------------\\n\", clr)\n    return errors, tests\nerrors, tests=predictor(test_gen, test_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T00:19:37.448958Z","iopub.execute_input":"2025-05-16T00:19:37.449564Z","iopub.status.idle":"2025-05-16T00:19:48.190836Z","shell.execute_reply.started":"2025-05-16T00:19:37.449542Z","shell.execute_reply":"2025-05-16T00:19:48.190071Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step\nthere were 16 errors in 200 tests for an accuracy of  92.00\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x800 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Classification Report:\n----------------------\n               precision    recall  f1-score   support\n\n    autistic     0.9375    0.9000    0.9184       100\nnon_autistic     0.9038    0.9400    0.9216       100\n\n    accuracy                         0.9200       200\n   macro avg     0.9207    0.9200    0.9200       200\nweighted avg     0.9207    0.9200    0.9200       200\n\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"working_dir=r'/kaggle/working/'\nsubject='autism-B5' \nacc=str(( 1-errors/tests) * 100)\nindex=acc.rfind('.')\nacc=acc[:index + 3]\nsave_id= subject + '_' + str(acc) + '.h5' \nmodel_save_loc=os.path.join(working_dir, save_id)\nmodel.save(model_save_loc)\nprint ('model was saved as ' , model_save_loc ) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T00:19:54.428788Z","iopub.execute_input":"2025-05-16T00:19:54.429260Z","iopub.status.idle":"2025-05-16T00:19:55.505804Z","shell.execute_reply.started":"2025-05-16T00:19:54.429238Z","shell.execute_reply":"2025-05-16T00:19:55.505154Z"}},"outputs":[{"name":"stdout","text":"model was saved as  /kaggle/working/autism-B5_92.0.h5\n","output_type":"stream"}],"execution_count":56}]}